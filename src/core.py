"""core.py

Pairs/spread analysis utilities with a production-friendly focus.

Key features
- Robust close-price loading from a yfinance-style CSV (MultiIndex header).
- Explicit calendar alignment (critical when mixing 24/7 assets with session-based markets).
- Spread construction as a regression residual in log-price space.
- Time-varying hedge ratio estimation via a lightweight Kalman filter.
- Adaptive z-score via exponentially weighted mean/std.
- Optional diagnostics: half-life estimate, ADF/KPSS tests (requires statsmodels).

Design notes
- The core pipeline only requires numpy + pandas.
- Optional dependencies (yfinance, statsmodels) are imported lazily inside functions.
"""

from __future__ import annotations

from pathlib import Path
from typing import Iterable, Optional, Tuple, Literal

import numpy as np
import pandas as pd


# -----------------------------------------------------------------------------
# Data I/O
# -----------------------------------------------------------------------------

def download_data(
    tickers: Iterable[str] | str,
    start: str,
    end: str,
    interval: str = "1d",
    save_path: str = "data/raw/prices.csv",
    auto_adjust: bool = True,
) -> pd.DataFrame:
    """Download OHLCV data with yfinance and save it to CSV.

    Notes
    -----
    - yfinance is imported lazily to keep CSV-only workflows lightweight.
    - The resulting CSV format is compatible with load_and_prepare_closes().
    """
    try:
        import yfinance as yf  # lazy import
    except Exception as e:
        raise ImportError(
            "yfinance is not installed. Install it with `pip install yfinance` "
            "or load an existing CSV via load_and_prepare_closes()."
        ) from e

    df = yf.download(
        tickers,
        start=start,
        end=end,
        interval=interval,
        auto_adjust=auto_adjust,
    )
    Path(save_path).parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(save_path)
    return df


def load_and_prepare_closes(
    path: str = "data/raw/prices.csv",
    preferred_fields: Tuple[str, ...] = ("Close", "Adj Close", "Price"),
    align: Literal["inner", "ffill", "none"] = "inner",
) -> pd.DataFrame:
    """Load a CSV (typically generated by yfinance) and return close prices.

    Parameters
    ----------
    path:
        CSV path.
    preferred_fields:
        Which top-level fields to prefer when the CSV uses a MultiIndex header.
    align:
        - "inner": keep only timestamps where all columns are present (recommended for pairs).
        - "ffill": forward-fill missing values (use with care; can distort 24/7 vs session mixes).
        - "none": keep NaNs (useful for diagnosing missingness).

    Returns
    -------
    pd.DataFrame
        Index: datetime, Columns: tickers, Values: close prices (float).
    """
    try:
        df = pd.read_csv(path, header=[0, 1], index_col=0, parse_dates=True)
        multi = isinstance(df.columns, pd.MultiIndex)
    except ValueError:
        df = pd.read_csv(path, header=0, index_col=0, parse_dates=True)
        multi = False

    if multi:
        top = df.columns.get_level_values(0)
        field = next((f for f in preferred_fields if f in top), None)
        if field is None:
            raise KeyError(f"None of {preferred_fields} found. Available: {top.unique().tolist()}")
        closes = df[field].copy()
        closes.columns.name = None
    else:
        # Flat CSV: look for columns like Close.BTC-USD
        field = next((f for f in preferred_fields if any(str(c).startswith(f) for c in df.columns)), None)
        if field is None:
            raise KeyError(f"None of {preferred_fields} found. Available: {list(df.columns)}")
        closes = df[[c for c in df.columns if str(c).startswith(field)]].copy()
        closes.columns = [str(c).replace(f"{field}.", "") for c in closes.columns]

    closes = closes.sort_index()

    if align == "inner":
        closes = closes.dropna()
    elif align == "ffill":
        closes = closes.ffill().dropna()
    elif align == "none":
        pass
    else:
        raise ValueError("align must be one of: 'inner', 'ffill', 'none'.")

    return closes


# -----------------------------------------------------------------------------
# Transformations
# -----------------------------------------------------------------------------

def compute_log_prices(closes: pd.DataFrame) -> pd.DataFrame:
    """Log-transform prices."""
    return np.log(closes)


def compute_log_returns(closes: pd.DataFrame) -> pd.DataFrame:
    """Compute log returns: log(P_t / P_{t-1})."""
    rets = np.log(closes / closes.shift(1))
    return rets.dropna()


def rolling_correlation_dual(
    returns: pd.DataFrame,
    asset_x: str,
    asset_y: str,
    window_short: int = 30,
    window_medium: int = 90,
    window_long: int = 180,
) -> pd.DataFrame:
    """Rolling Pearson correlations for three windows (diagnostic only)."""
    pair = returns[[asset_x, asset_y]].dropna()
    return pd.DataFrame(
        {
            f"corr_{window_short}": pair[asset_x].rolling(window_short).corr(pair[asset_y]),
            f"corr_{window_medium}": pair[asset_x].rolling(window_medium).corr(pair[asset_y]),
            f"corr_{window_long}": pair[asset_x].rolling(window_long).corr(pair[asset_y]),
        }
    )


# -----------------------------------------------------------------------------
# Spread construction
# -----------------------------------------------------------------------------

def spread_residual_ols(
    log_prices: pd.DataFrame,
    asset_y: str,
    asset_x: str,
) -> tuple[pd.Series, float, float]:
    """OLS residual spread in log-price space.

    Model
    -----
    log(Y_t) = alpha + beta * log(X_t) + eps_t
    spread_t = eps_t

    Returns
    -------
    spread: pd.Series
    alpha: float
    beta: float
    """
    data = log_prices[[asset_y, asset_x]].dropna()
    y = data[asset_y].to_numpy(dtype=float)
    x = data[asset_x].to_numpy(dtype=float)

    X = np.column_stack([np.ones(len(x)), x])
    alpha, beta = np.linalg.lstsq(X, y, rcond=None)[0]

    spread = data[asset_y] - (alpha + beta * data[asset_x])
    return spread, float(alpha), float(beta)


def kalman_hedge_ratio(
    log_prices: pd.DataFrame,
    asset_y: str,
    asset_x: str,
    delta: float = 1e-5,
    r: float = 1e-3,
    include_intercept: bool = True,
) -> pd.DataFrame:
    """Time-varying hedge ratio via a simple Kalman filter.

    State-space form
    ----------------
    y_t = alpha_t + beta_t * x_t + v_t
    theta_t = theta_{t-1} + w_t

    Parameters
    ----------
    delta:
        Process noise scale (higher => alpha/beta adapt faster).
    r:
        Observation noise (higher => smoother estimates).
    include_intercept:
        If False, forces alpha = 0 and only estimates beta.

    Returns
    -------
    pd.DataFrame
        Columns: alpha, beta, spread (residual)
    """
    data = log_prices[[asset_y, asset_x]].dropna()
    y = data[asset_y].to_numpy(dtype=float)
    x = data[asset_x].to_numpy(dtype=float)

    n = 2 if include_intercept else 1
    theta = np.zeros(n)           # [alpha, beta] or [beta]
    P = np.eye(n) * 1e5           # large initial uncertainty
    Q = np.eye(n) * float(delta)  # process noise

    alphas = np.full(len(data), np.nan)
    betas = np.full(len(data), np.nan)
    spreads = np.full(len(data), np.nan)

    for t in range(len(data)):
        # Predict
        P = P + Q

        F = np.array([1.0, x[t]]) if include_intercept else np.array([x[t]])
        y_hat = float(F @ theta)
        e = y[t] - y_hat

        S = float(F @ P @ F.T + r)     # innovation variance
        K = (P @ F.T) / S              # Kalman gain

        # Update
        theta = theta + K * e
        P = P - np.outer(K, F) @ P

        if include_intercept:
            alphas[t] = theta[0]
            betas[t] = theta[1]
            spreads[t] = y[t] - (theta[0] + theta[1] * x[t])
        else:
            alphas[t] = 0.0
            betas[t] = theta[0]
            spreads[t] = y[t] - (theta[0] * x[t])

    return pd.DataFrame({"alpha": alphas, "beta": betas, "spread": spreads}, index=data.index)


# -----------------------------------------------------------------------------
# Adaptive z-score
# -----------------------------------------------------------------------------

def zscore_ewm(
    series: pd.Series,
    span: int = 60,
    min_periods: Optional[int] = None,
) -> pd.Series:
    """Adaptive z-score using exponentially weighted mean/std."""
    if min_periods is None:
        min_periods = span

    mean = series.ewm(span=span, adjust=False, min_periods=min_periods).mean()
    std = series.ewm(span=span, adjust=False, min_periods=min_periods).std(bias=False)
    std = std.replace(0, np.nan)

    z = (series - mean) / std
    return z.replace([np.inf, -np.inf], np.nan)


# -----------------------------------------------------------------------------
# Diagnostics
# -----------------------------------------------------------------------------

def half_life(series: pd.Series) -> float:
    """Estimate mean-reversion half-life (approx.) from an AR(1)-style fit.

    We fit:
        Î”s_t = a + gamma * s_{t-1} + eps
    If gamma < 0, the process mean-reverts and half-life is approximated by:
        HL = ln(2) / (-ln(1 + gamma))

    Returns
    -------
    float
        Half-life in bars. Returns inf when not mean-reverting or when estimation is unreliable.
    """
    s = series.dropna()
    if len(s) < 30:
        return float("inf")

    lag = s.shift(1).dropna()
    ds = s.diff().dropna()
    lag = lag.loc[ds.index]

    X = np.column_stack([np.ones(len(lag)), lag.to_numpy(dtype=float)])
    _, gamma = np.linalg.lstsq(X, ds.to_numpy(dtype=float), rcond=None)[0]

    if gamma >= 0 or np.isclose(1 + gamma, 0):
        return float("inf")

    try:
        return float(np.log(2) / (-np.log(1 + gamma)))
    except Exception:
        return float("inf")


def adf_test(series: pd.Series, autolag: str = "AIC") -> dict:
    """Augmented Dickey-Fuller test (requires statsmodels)."""
    try:
        from statsmodels.tsa.stattools import adfuller
    except Exception as e:
        raise ImportError("statsmodels is required for adf_test: pip install statsmodels") from e

    s = series.dropna().to_numpy(dtype=float)
    stat, pval, *_ = adfuller(s, autolag=autolag)
    return {"adf_stat": float(stat), "pvalue": float(pval)}


def kpss_test(series: pd.Series, regression: str = "c", nlags: str = "auto") -> dict:
    """KPSS test (requires statsmodels)."""
    try:
        from statsmodels.tsa.stattools import kpss
    except Exception as e:
        raise ImportError("statsmodels is required for kpss_test: pip install statsmodels") from e

    s = series.dropna().to_numpy(dtype=float)
    stat, pval, *_ = kpss(s, regression=regression, nlags=nlags)
    return {"kpss_stat": float(stat), "pvalue": float(pval)}
